---
title: Semana 1 de Mayo
authors: santiago
tags: [Informe de actividades]
---

## Estado proyecto del proyecto de introducción a la IA en SINTECTO e investigación relacionada con IA 

En la sección de Asistentes de la documentación ([https://saanrm.github.io/notas-sintecto/category/ia](https://saanrm.github.io/notas-sintecto/category/ia)), se ha ampliado el análisis del uso de distintos enfoques para implementar chatbots. A diferencia de los “prompts planos”, en los que el desarrollador debe encargarse manualmente de cada paso —desde la gestión del historial de mensajes y su almacenamiento, hasta la actualización de contexto y la serialización de cada intercambio— los Asistentes de OpenAI ofrecen una capa de abstracción que simplifica enormemente este flujo de trabajo.

Por ejemplo, con un asistente podemos:

* **Definir roles de forma nativa** (`system`, `user`, `assistant`), de modo que ya no hace falta insertar manualmente instrucciones de sistema o concatenar todas las entradas de usuario cada vez que iniciamos una llamada.
* **Gestionar el historial de conversación**, delegando en la API la unión y el recorte inteligente del contexto cuando se supera el límite de tokens.
* **Actualizar o inyectar memoria** de manera programática, para que la conversación pueda “recordar” datos previos sin tener que reconstruirlos manualmente en cada petición.
* **Orquestar múltiples flujos de trabajo** (por ejemplo, llamar a funciones externas, hacer comprobaciones de validación o guardar logs) mediante las herramientas de “function calling”, lo cual hace que la lógica de negocio quede más limpia y desacoplada del prompt en sí.
* **Optimizar la parametrización** de temperatura, top\_p o límites de longitud para cada tipo de usuario o escenario, sin tener que reescribir siempre el mismo bloque de texto.

En resumen, al usar Asistentes de OpenAI tenemos una “estructura prehecha” que permite:

1. Separar claramente la capa de instrucciones generales (sistema) de las interacciones de usuario.
2. Delegar en la API la administración del tamaño y relevancia del contexto.
3. Integrar de forma nativa llamadas a funciones, registros de eventos y persistencia de datos.
4. Mantener nuestro código más limpio, reutilizable y mantenible, reduciendo la complejidad de añadir nuevas funcionalidades al chatbot.

Por todo esto, para el desarrollo de un chatbot robusto y escalable, emplear Asistentes ofrece un camino más rápido y ordenado que trabajar únicamente con “un flujo de prompts normales”.

### Costo

Los costos de usar Asistentes o prompts tradicionales son idénticos, pues los tokens de entrada y salida se facturan de la misma forma y no existe ningún cargo adicional por emplear Asistentes. No obstante, conservar el historial o el contexto previo de la conversación aumenta inevitablemente el consumo de tokens (y, por tanto, el coste), sea cual sea el enfoque elegido.

### Continuación

Si bien el chatbot funciona correctamente en su forma actual, lo más óptimo sería refactorizar el código para adoptar el paradigma de Asistentes de OpenAI; esto le aportaría un diseño más sólido, escalable y mantenible, facilitando a largo plazo la incorporación de nuevas funcionalidades o la edición del sistema de manera más sencilla y segura. Además, para continuar con el aprendizaje y la expansión de los conocimientos de IA.

## **Tipos de desarrollos según la necesidad de historial**

:::danger
Importante
:::

Basado en la investigación realizada, se pueden catalogar dos grandes tipos de desarrollos con LLMs:

1. **Desarrollos que requieren historial**

   * **Definición:** Módulos que necesitan conservar el registro completo (o recortado) de las entradas y salidas previas para ofrecer continuidad y coherencia en la conversación.
   * **Ejemplo típico:** Un chatbot conversacional, donde cada mensaje del usuario y cada respuesta del modelo se añaden al historial.
   * **Implicaciones en tokens y costos:**

     * A cada llamada a la API se envían tanto el mensaje actual como todo el historial relevante.
     * El conteo de tokens crece de forma acumulativa: al historial anterior se suman los tokens de entrada nuevos y los de salida.
     * Esto puede disparar el consumo de tokens y, por tanto, los costos, especialmente en diálogos largos o muy activos.

2. **Desarrollos que no requieren historial**

   * **Definición:** Módulos en los que cada invocación al modelo es independiente y no hace falta aportar contexto de interacciones anteriores.
   * **Ejemplo típico:** Un prompt puntual para obtener un dato o realizar una operación específica (p. ej. “¿Cuál es la capital de Colombia?” → “Bogotá”).
   * **Implicaciones en tokens y costos:**

     * Sólo se cuentan los tokens de la consulta actual y la respuesta inmediata.
     * El consumo de tokens es mucho más predecible y bajo, resultando en menores costes por llamada.

:::danger
**Recomendación**: Antes de diseñar e implementar un módulo con LLMs, es fundamental entender si el caso de uso necesita mantener contexto (historial) o puede operar con prompts independientes. Esta decisión impactará directamente en la arquitectura, en el estilo de integración (Asistentes vs. prompts planos) y en el coste operativo a largo plazo.
:::